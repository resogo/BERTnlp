{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import random\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "\n",
    "from glob import glob\n",
    "from tensorflow.keras.utils import Progbar\n",
    "\n",
    "LANG_CODE = \"en\" #@param {type:\"string\"}\n",
    "\n",
    "regex_tokenizer = nltk.RegexpTokenizer(\"\\w+\")\n",
    "\n",
    "def normalize_text(text):\n",
    "  # lowercase text\n",
    "  text = str(text).lower()\n",
    "  # remove non-UTF\n",
    "  text = text.encode(\"utf-8\", \"ignore\").decode()\n",
    "  # remove punktuation symbols\n",
    "  text = \" \".join(regex_tokenizer.tokenize(text))\n",
    "  return text\n",
    "\n",
    "def count_lines(filename):\n",
    "  count = 0\n",
    "  with open(filename) as fi:\n",
    "    for line in fi:\n",
    "      count += 1\n",
    "  return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "odata_path = \".\\pretrain_data_format\\original.txt\"\n",
    "file_object  = open(odata_path, \"r+\")\n",
    "odata_contents = file_object.read()\n",
    "file_object.close()\n",
    "odata_contents = re.sub(\"START_OF_RECORD=.+||||\",\"\",odata_contents)\n",
    "odata_new = re.sub(\"||||END_OF_RECORD\",\"\",odata_contents)\n",
    "file = open(\".\\pretrain_data_format\\dataset.txt\",\"w\") \n",
    "file.write(odata_new)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35179/35179 [==============================] - 0s 10us/step\n"
     ]
    }
   ],
   "source": [
    "RAW_DATA_FPATH = \".\\pretrain_data_format\\dataset.txt\" #@param {type: \"string\"}\n",
    "PRC_DATA_FPATH = \".\\pretrain_data_format\\proc_dataset.txt\" #@param {type: \"string\"}\n",
    "\n",
    "# apply normalization to the dataset\n",
    "# this will take a minute or two\n",
    "\n",
    "total_lines = count_lines(RAW_DATA_FPATH)\n",
    "bar = Progbar(total_lines)\n",
    "\n",
    "with open(RAW_DATA_FPATH,encoding=\"utf-8\") as fi:\n",
    "  with open(PRC_DATA_FPATH, \"w\",encoding=\"utf-8\") as fo:\n",
    "    for l in fi:\n",
    "      fo.write(normalize_text(l)+\"\\n\")\n",
    "      bar.add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PREFIX = \"tokenizer\" #@param {type: \"string\"}\n",
    "VOC_SIZE = 7000 #@param {type:\"integer\"}\n",
    "SUBSAMPLE_SIZE = 12800000 #@param {type:\"integer\"}\n",
    "NUM_PLACEHOLDERS = 256 #@param {type:\"integer\"}\n",
    "\n",
    "SPM_COMMAND = ('--input={} --model_prefix={} '\n",
    "               '--vocab_size={} --input_sentence_size={} '\n",
    "               '--shuffle_input_sentence=true ' \n",
    "               '--bos_id=-1 --eos_id=-1').format(\n",
    "               PRC_DATA_FPATH, MODEL_PREFIX, \n",
    "               VOC_SIZE - NUM_PLACEHOLDERS, SUBSAMPLE_SIZE)\n",
    "\n",
    "spm.SentencePieceTrainer.Train(SPM_COMMAND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnt vocab size: 6743\n",
      "Sample tokens: ['rated', '▁dress', '▁proxy', '▁stimuli', '▁pad', '▁limit', '▁premed', '▁tips', '▁deep', 'riction']\n"
     ]
    }
   ],
   "source": [
    "def read_sentencepiece_vocab(filepath):\n",
    "  voc = []\n",
    "  with open(filepath, encoding='utf-8') as fi:\n",
    "    for line in fi:\n",
    "      voc.append(line.split(\"\\t\")[0])\n",
    "  # skip the first <unk> token\n",
    "  voc = voc[1:]\n",
    "  return voc\n",
    "\n",
    "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format(MODEL_PREFIX))\n",
    "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
    "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n"
     ]
    }
   ],
   "source": [
    "def parse_sentencepiece_token(token):\n",
    "    if token.startswith(\"▁\"):\n",
    "        return token[1:]\n",
    "    else:\n",
    "        return \"##\" + token\n",
    "        \n",
    "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))\n",
    "\n",
    "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
    "bert_vocab = ctrl_symbols + bert_vocab\n",
    "\n",
    "bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(VOC_SIZE - len(bert_vocab))]\n",
    "print(len(bert_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
    "\n",
    "with open(VOC_FNAME, \"w\") as fo:\n",
    "  for token in bert_vocab:\n",
    "    fo.write(token+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['essent',\n",
       " '##ial',\n",
       " '##ly',\n",
       " '[UNK]',\n",
       " 'mr',\n",
       " '[UNK]',\n",
       " 'cor',\n",
       " '##ne',\n",
       " '##a',\n",
       " 'is',\n",
       " 'a',\n",
       " '60',\n",
       " 'year',\n",
       " 'old',\n",
       " 'mal',\n",
       " '##e',\n",
       " 'who',\n",
       " 'noted',\n",
       " 'the',\n",
       " 'on',\n",
       " '##set',\n",
       " 'of',\n",
       " 'dark',\n",
       " 'urine',\n",
       " 'during',\n",
       " 'early',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " 'he',\n",
       " 'under',\n",
       " '##went',\n",
       " 'ct',\n",
       " 'and',\n",
       " 'ercp',\n",
       " 'at',\n",
       " 'the',\n",
       " 'lis',\n",
       " '##ona',\n",
       " '##tem',\n",
       " '##i',\n",
       " 'fa',\n",
       " '##yl',\n",
       " '##and',\n",
       " '##sb',\n",
       " '##urg',\n",
       " '##nic',\n",
       " '[UNK]',\n",
       " 'commun',\n",
       " '##ity',\n",
       " 'hospital',\n",
       " 'with',\n",
       " 'a',\n",
       " 'stent',\n",
       " 'placement',\n",
       " 'and',\n",
       " 'resolution',\n",
       " 'of',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " 'he',\n",
       " 'under',\n",
       " '##went',\n",
       " 'an',\n",
       " 'echo',\n",
       " 'and',\n",
       " 'endoscop',\n",
       " '##y',\n",
       " 'at',\n",
       " 'in',\n",
       " '##g',\n",
       " '##re',\n",
       " '##e',\n",
       " 'and',\n",
       " 'ot',\n",
       " 'of',\n",
       " 'wea',\n",
       " '##man',\n",
       " '##sh',\n",
       " '##y',\n",
       " 'medical',\n",
       " 'center',\n",
       " 'on',\n",
       " 'ap',\n",
       " '##r',\n",
       " '##il',\n",
       " '28',\n",
       " '[UNK]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path.append(\"bert\")\n",
    "from bert import modeling, optimization, tokenization\n",
    "from bert.run_pretraining import input_fn_builder, model_fn_builder\n",
    "testcase = \" Essentially , Mr. Cornea is a 60 year old male who noted the onset of dark urine during early January . He underwent CT and ERCP at the Lisonatemi Faylandsburgnic, Community Hospital with a stent placement and resolution of jaundice . He underwent an ECHO and endoscopy at Ingree and Ot of Weamanshy Medical Center on April 28.\"\n",
    "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
    "bert_tokenizer.tokenize(testcase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocab File is created and we can now make the pre-training data with create_pretraining_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 50 #@param {type:\"integer\"}\n",
    "MASKED_LM_PROB = 0.15 #@param\n",
    "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
    "DO_LOWER_CASE = True #@param {type:\"boolean\"}\n",
    "\n",
    "PRETRAINING_DIR = \"pretraining_data_dir\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gfile.MkDir(PRETRAINING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"from_scratch_BERT\"\n",
    "tf.gfile.MkDir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this for BERT-base\n",
    "\n",
    "bert_base_config = {\n",
    "  \"attention_probs_dropout_prob\": 0.1, \n",
    "  \"directionality\": \"bidi\", \n",
    "  \"hidden_act\": \"gelu\", \n",
    "  \"hidden_dropout_prob\": 0.1, \n",
    "  \"hidden_size\": 768, \n",
    "  \"initializer_range\": 0.02, \n",
    "  \"intermediate_size\": 3072, \n",
    "  \"max_position_embeddings\": 512, \n",
    "  \"num_attention_heads\": 12, \n",
    "  \"num_hidden_layers\": 12, \n",
    "  \"pooler_fc_size\": 768, \n",
    "  \"pooler_num_attention_heads\": 12, \n",
    "  \"pooler_num_fc_layers\": 3, \n",
    "  \"pooler_size_per_head\": 128, \n",
    "  \"pooler_type\": \"first_token_transform\", \n",
    "  \"type_vocab_size\": 2, \n",
    "  \"vocab_size\": VOC_SIZE\n",
    "}\n",
    "\n",
    "with open(\"{}/bert_config.json\".format(MODEL_DIR), \"w\") as fo:\n",
    "  json.dump(bert_base_config, fo, indent=2)\n",
    "  \n",
    "with open(\"{}/{}\".format(MODEL_DIR, VOC_FNAME), \"w\") as fo:\n",
    "  for token in bert_vocab:\n",
    "    fo.write(token+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
