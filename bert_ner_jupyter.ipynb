{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0728 10:58:18.745058 13360 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model output directory: .\\data_out *****\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = '.\\data_out'\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNOT NEEDED\\nimport csv\\ndef data_from_csv(input_file):\\n    file_CSV = open(input_file)\\n    data_CSV = csv.reader(file_CSV)\\n    list_CSV = list(data_CSV)\\n    reader = csv.reader(input_file,)\\n    lines = []; words = []; labels = [];\\n    for row in list_CSV[1:]:\\n        if (row[0]=='.'):\\n            l = ' '.join([label for label in labels if len(label) > 0])\\n            w = ' '.join([word for word in words if len(word) > 0])\\n            words=[]\\n            labels = []\\n            lines.append((w, l))\\n        else:\\n            words.append(row[0])\\n            labels.append(row[1])\\n    return lines\\n    \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "NOT NEEDED\n",
    "import csv\n",
    "def data_from_csv(input_file):\n",
    "    file_CSV = open(input_file)\n",
    "    data_CSV = csv.reader(file_CSV)\n",
    "    list_CSV = list(data_CSV)\n",
    "    reader = csv.reader(input_file,)\n",
    "    lines = []; words = []; labels = [];\n",
    "    for row in list_CSV[1:]:\n",
    "        if (row[0]=='.'):\n",
    "            l = ' '.join([label for label in labels if len(label) > 0])\n",
    "            w = ' '.join([word for word in words if len(word) > 0])\n",
    "            words=[]\n",
    "            labels = []\n",
    "            lines.append((w, l))\n",
    "        else:\n",
    "            words.append(row[0])\n",
    "            labels.append(row[1])\n",
    "    return lines\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = data_from_csv('./NER/train.csv')\n",
    "#dev = data_from_csv('./NER/dev.csv')\n",
    "#test = data_from_csv('./NER/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "train = pandas.read_csv('./NER/train.csv')\n",
    "train = train.iloc[1:4988,:]\n",
    "\n",
    "test = pandas.read_csv('./NER/dev.csv')\n",
    "test = test.iloc[1:4968,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def make_sentences(input_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "                rf = list(csv.reader(f))\n",
    "                lines = [];words = [];labels = []\n",
    "                for line in rf[1:]:\n",
    "                    word = line[0]\n",
    "                    label = line[1]\n",
    "                    # here we dont do \"DOCSTART\" check\n",
    "                    if len(word)<=1 and word[-1] == '.':\n",
    "                        l = ' '.join([label for label in labels if len(label) > 0])\n",
    "                        w = ' '.join([word for word in words if len(word) > 0])\n",
    "                        lines.append((w,l))\n",
    "                        words=[]\n",
    "                        labels = []\n",
    "                    else:\n",
    "                        words.append(word)\n",
    "                        labels.append(label)\n",
    "    return lines\n",
    "train = make_sentences('./NER/train.csv')\n",
    "train = pd.DataFrame(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       0  \\\n",
      "0      113416550 PRGH 13523357 630190 6/7 1999 12:00:...   \n",
      "1      He underwent CT and ERCP at the Lisonatemi Fay...   \n",
      "2      He underwent an ECHO and endoscopy at Ingree a...   \n",
      "3      He was found to have a large , bulging , extri...   \n",
      "4      Fine needle aspiration showed atypical cells ,...   \n",
      "5      Abdominal CT on April 14 , showed a 12 x 8 x 8...   \n",
      "6      He denied any nausea , vomiting , anorexia , o...   \n",
      "7      He states that his color in urine or in stool ...   \n",
      "8      PAST MEDICAL HISTORY : He has hypertension and...   \n",
      "9      PAST SURGICAL HISTORY : Status post left kidne...   \n",
      "10            ALLERGIES : He has no known drug allergies   \n",
      "11     MEDICATIONS PRIOR TO ADMISSION : Hydrochloroth...   \n",
      "12     He had an uncomplicated postoperative course a...   \n",
      "13     Advanced his diet on postop day # 4 to a trans...   \n",
      "14     His PCA was discontinued on postop day # 4 , a...   \n",
      "15                  PHYSICAL EXAMINATION : CHEST : Clear   \n",
      "16                                       HEART : Regular   \n",
      "17           ABDOMINAL INCISION : Clean , dry and intact   \n",
      "18                                           No drainage   \n",
      "19     VITAL SIGNS : He is afebrile and otherwise vit...   \n",
      "20     He is having good p.o. intake on present diet ...   \n",
      "21     DISPOSITION : He is going to a rehabilitation ...   \n",
      "22     DISCHARGE MEDICATIONS : Same as pre-op , with ...   \n",
      "23     Dictated By : THAMETO DOYLE , M.D. OS43 Attend...   \n",
      "24     He underwent CT and ERCP at the Lisonatemi Fay...   \n",
      "25     He underwent an ECHO and endoscopy at Ingree a...   \n",
      "26     He was found to have a large , bulging , extri...   \n",
      "27     Fine needle aspiration showed atypical cells ,...   \n",
      "28     Abdominal CT on April 14 , showed a 12 x 8 x 8...   \n",
      "29     He denied any nausea , vomiting , anorexia , o...   \n",
      "...                                                  ...   \n",
      "18157  She tolerated this procedure without complicat...   \n",
      "18158  Postoperatively , she had her heparin restarte...   \n",
      "18159  Her primary postoperative development was the ...   \n",
      "18160  She continued to pass blood per rectum and was...   \n",
      "18161  Her preoperative hematocrit was 41 and her hem...   \n",
      "18162  This was a drop from an immediate postoperativ...   \n",
      "18163  Upon receiving the one unit of red blood cells...   \n",
      "18164  Otherwise , her postoperative course was uneve...   \n",
      "18165  Her heparin was discontinued when it was noted...   \n",
      "18166  On postoperative day # 1 , the patient was res...   \n",
      "18167  However , due to the episode of bleeding on po...   \n",
      "18168  At this time , she is given 3 mg and was gradu...   \n",
      "18169  At the time of discharge , it was 1.9 , for wh...   \n",
      "18170  This was postoperative day # 6 , at which time...   \n",
      "18171  The rest of her hospital course was unremarkab...   \n",
      "18172      She had some initial nausea but had no emesis   \n",
      "18173  Her nasogastric tube was discontinued on posto...   \n",
      "18174  She had flatus and regular bowel movements by ...   \n",
      "18175  She also complained of some slight dizziness u...   \n",
      "18176  For this reason , her normal Lopressor and Nor...   \n",
      "18177  It was felt that this was re-equalibration aft...   \n",
      "18178  She had some slight headaches which resolved w...   \n",
      "18179  She was ready for discharge on postoperative d...   \n",
      "18180  Her wound remained benign and was healing nice...   \n",
      "18181  DISCHARGE MEDICATIONS : Lopressor 50 mg q.a.m....   \n",
      "18182  DISCHARGE FOLLOW-UP : Follow-up with Dr. Jesc ...   \n",
      "18183  DISCHARGE DISPOSITION : Her disposition was to...   \n",
      "18184                    CONDITION ON DISCHARGE : Stable   \n",
      "18185                      Estimated disability was none   \n",
      "18186  Dictated By : BELL REXBEATHEFARST , M.D. NM82 ...   \n",
      "\n",
      "                                                       1  \n",
      "0      ID HOSPITAL ID ID DATE NONE NONE NONE NONE NON...  \n",
      "1      NONE NONE NONE NONE NONE NONE NONE HOSPITAL NO...  \n",
      "2      NONE NONE NONE NONE NONE NONE NONE HOSPITAL NO...  \n",
      "3      NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "4      NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "5      NONE NONE NONE DATE NONE NONE NONE NONE NONE N...  \n",
      "6      NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "7      NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "8           NONE NONE NONE NONE NONE NONE NONE NONE NONE  \n",
      "9      NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "10               NONE NONE NONE NONE NONE NONE NONE NONE  \n",
      "11     NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "12     NONE NONE NONE NONE NONE NONE NONE NONE NONE NONE  \n",
      "13     NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "14     NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "15                         NONE NONE NONE NONE NONE NONE  \n",
      "16                                        NONE NONE NONE  \n",
      "17               NONE NONE NONE NONE NONE NONE NONE NONE  \n",
      "18                                             NONE NONE  \n",
      "19     NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "20     NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "21     NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "22     NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "23     NONE NONE NONE DOCTOR NONE NONE ID NONE NONE D...  \n",
      "24     NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "25     NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "26     NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "27     NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "28     NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "29     NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "...                                                  ...  \n",
      "18157  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18158  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18159  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18160  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18161  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18162  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18163  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18164  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18165  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18166  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18167  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18168  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18169  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18170  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18171  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18172       NONE NONE NONE NONE NONE NONE NONE NONE NONE  \n",
      "18173  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18174  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18175  NONE NONE NONE NONE NONE NONE NONE NONE NONE NONE  \n",
      "18176  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18177  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18178  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18179  NONE NONE NONE NONE NONE NONE NONE NONE NONE NONE  \n",
      "18180  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18181  NONE NONE NONE NONE NONE NONE NONE NONE NONE N...  \n",
      "18182  NONE NONE NONE NONE NONE NONE DOCTOR NONE NONE...  \n",
      "18183            NONE NONE NONE NONE NONE NONE NONE NONE  \n",
      "18184                           NONE NONE NONE NONE NONE  \n",
      "18185                                NONE NONE NONE NONE  \n",
      "18186  NONE NONE NONE DOCTOR NONE NONE ID NONE NONE D...  \n",
      "\n",
      "[18187 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_col = 0\n",
    "label_col = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = \"[PAD]\", \"PATIENT\", \"DATE\", \"HOSPITAL\", \"PHONE\", \"DOCTOR\", \"LOCATION\", \"AGE\", \"ID\", \"NONE\", \"X\", \"[CLS]\",\"[SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[data_col], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[label_col]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rebec\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 11:03:19.973137 13360 deprecation.py:323] From C:\\Users\\rebec\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0728 11:03:22.898733 13360 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import pickle\n",
    "from absl import flags,logging\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def filed_based_convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, output_file,mode=None):\n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "    batch_tokens = []\n",
    "    batch_labels = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 5000 == 0:\n",
    "            logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))  \n",
    "        feature,ntokens,label_ids = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, mode)\n",
    "        batch_tokens.extend(ntokens)\n",
    "        batch_labels.extend(label_ids)\n",
    "        def create_int_feature(values):\n",
    "            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "            return f\n",
    "\n",
    "        features = collections.OrderedDict()\n",
    "        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "        features[\"mask\"] = create_int_feature(feature.mask)\n",
    "        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "        features[\"label_ids\"] = create_int_feature(feature.label_ids)\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    # sentence token in each batch\n",
    "    writer.close()\n",
    "    return batch_tokens,batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'InputExample' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-cb2319aae11b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moutput_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"output_file.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Convert our train and test features to InputFeatures that BERT understands.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiled_based_convert_examples_to_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_InputExamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_SEQ_LENGTH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-93d0a069b586>\u001b[0m in \u001b[0;36mfiled_based_convert_examples_to_features\u001b[1;34m(examples, label_list, max_seq_length, tokenizer, output_file, mode)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mex_index\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m5000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Writing example %d of %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mex_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mntokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_single_example\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mbatch_tokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mntokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mbatch_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-0c7bc79f8cd2>\u001b[0m in \u001b[0;36mconvert_single_example\u001b[1;34m(ex_index, example, label_list, max_seq_length, tokenizer, mode)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m#with open(FLAGS.middle_output+\"/label2id.pkl\",'wb') as w:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m#    pickle.dump(label_map,w)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mtextlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mlabellist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'InputExample' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "output_file = \"output_file.txt\"\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = filed_based_convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "  bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "  bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "  bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "  # Use \"sequence_outputs\" for token-level output.\n",
    "  output_layer = bert_outputs[\"sequence_output\"]\n",
    "\n",
    "  hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "  # Create our own layer to tune for politeness data.\n",
    "  output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "  with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    # Dropout helps prevent overfitting\n",
    "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    # Convert labels into one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "    if is_predicting:\n",
    "      return (predicted_labels, log_probs)\n",
    "\n",
    "    # If we're train/eval, compute loss between predicted and actual label\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    return (loss, predicted_labels, log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "    # TRAIN and EVAL\n",
    "    if not is_predicting:\n",
    "\n",
    "      (loss, predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      train_op = bert.optimization.create_optimizer(\n",
    "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "      # Calculate evaluation metrics. \n",
    "      def metric_fn(label_ids, predicted_labels):\n",
    "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "        f1_score = tf.contrib.metrics.f1_score(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        auc = tf.metrics.auc(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        recall = tf.metrics.recall(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        precision = tf.metrics.precision(\n",
    "            label_ids,\n",
    "            predicted_labels) \n",
    "        true_pos = tf.metrics.true_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        true_neg = tf.metrics.true_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)   \n",
    "        false_pos = tf.metrics.false_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)  \n",
    "        false_neg = tf.metrics.false_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"auc\": auc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"true_positives\": true_pos,\n",
    "            \"true_negatives\": true_neg,\n",
    "            \"false_positives\": false_pos,\n",
    "            \"false_negatives\": false_neg\n",
    "        }\n",
    "\n",
    "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "          loss=loss,\n",
    "          train_op=train_op)\n",
    "      else:\n",
    "          return tf.estimator.EstimatorSpec(mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metrics)\n",
    "    else:\n",
    "      (predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      predictions = {\n",
    "          'probabilities': log_probs,\n",
    "          'labels': predicted_labels\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "  # Return the actual model function in the closure\n",
    "  return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
